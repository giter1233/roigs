# roigs
README

Project Overview

This project builds upon the standard 3D Gaussian Splatting reconstruction pipeline by introducing ROI-based densification and pruning gating, as well as a dynamic Laplacian pyramid reconstruction loss. It integrates monocular depth estimation and segmentation masks to better balance fine detail fidelity in the target region and overall robustness. The code provides a complete pipeline for training, rendering, and metric evaluation, with TensorBoard logging and reproducible output structure.

## 1. Environment and Dependencies

- The project ships an environment configuration file and installs local submodules (diff-gaussian-rasterization, simple-knn, fused-ssim).

Create the environment and install dependencies:

```bash
# Run in the project root
conda env create -f environment.yml
conda activate roigs

# Install DepthPro Python package (enable the preprocessing script)
pip install -e depth/src

# Download DepthPro pretrained weights
bash depth/get_pretrained_models.sh
```

## 2. Data Preparation

The project supports data structures from COLMAP and Blender. For COLMAP, organize your dataset as:

```
<source_path>/
  images/              # input images (subdirectories supported)
  sparse/0/            # COLMAP sparse reconstruction (cameras.bin / images.bin / points3D.bin or corresponding .txt)
  pure_masks/          # optional: binary segmentation masks, same filename as images, .png/.jpg suffix
  depth_pro/           # optional: monocular depth maps generated by the preprocessing script (.npy)
```

- Mask naming rule: place binary masks in `pure_masks` with exactly the same filenames as the images. A threshold near 127 is fine; foreground should be white (255) and background black (0).
- Depth paths and naming: place `.npy` files in `depth_pro` with the same filenames as the images. The system will automatically match them, align resolution, and intersect with masks during loading.

## Mask Generation (SAM2)

To focus training on the target region and make densification/pruning more robust, generate a binary mask for each input image with the same filename and place it in `<source_path>/pure_masks`. SAM2 image segmentation scripts are integrated and support interactive annotation and batch propagation.

Setup and installation:
- In the project root, install SAM2 as an editable package:
  ```bash
  pip install --no-build-isolation -e sam2
  ```
- Put SAM2 weight files into the `checkpoints/` directory and configure the corresponding model weight path and config file path inside the scripts (e.g., `sam2.1_hiera_large.pt` and `configs/sam2.1/sam2.1_hiera_l.yaml`).

Path configuration (edit the Config at the top of the scripts to point to your dataset):
- `IMAGE_SEQ_DIR` = `<source_path>/images`
- `pure_mask_dir` = `<source_path>/pure_masks` (create the directory if missing)

Run:
- Interactive annotation (click foreground/background per image, then batch-generate masks; can optionally save overlays and crops):
  ```bash
  python sam2/segimage_interactive.py
  ```
- Preset single-target batch generation (generate masks directly per the target name and strategy in the script):
  ```bash
  python sam2/segimage.py
  ```

Output rules:
- Masks are saved to `<source_path>/pure_masks` with filenames identical to the original images, in `.png` or `.jpg` format. Foreground pixels: 255 (white); background: 0 (black).
- The scripts can also save semi-transparent overlay visualizations and crops based on masks for inspection and manual fine-tuning.

Notes:
- Ensure one-to-one matching between masks and images with strictly identical naming. If images are in subdirectories, keep the relative structure consistent or move masks to the `pure_masks` root after generation as needed.
- If mask boundaries show jagged edges or small speckles, you may apply light morphological operations (e.g., dilation/erosion). Training also provides soft-edge and slight dilation options to mitigate boundary errors.
- When both `pure_masks` and `depth_pro` are present, training automatically uses the intersection as the effective ROI to reduce outliers and long-range errors.

## Depth Preprocessing (DepthPro)

The project provides a monocular depth preprocessing script to batch-generate depth maps and metadata for your dataset.

```bash
# Generate <source_path>/depth_pro/*.npy and *_meta.npz
python predepth.py -s <source_path>
```

- Before running, ensure the DepthPro package is installed and weights are downloaded (see above).
- The script recursively traverses the `images` directory and saves results into `depth_pro`, preserving the same relative paths.

## Training

The training entry point in the project root supports basic settings (data root, output directory, resolution scale, etc.) and can enable ROI gating and the dynamic Laplacian pyramid loss.

```bash
# Basic training
python train.py 
```
```bash
# Enable ROI gating (combine masks and global depth radius) to focus on the target region
python train.py  -s <source_path> -m <output_dir> --use_mask 
```

The training output structure under `<output_dir>` creates reproducible experiment records (config snapshot, TensorBoard logs, weights, etc.). If ROI gating is enabled and both `pure_masks` and `depth_pro` are present, their intersection is used as the effective region, affecting densification/pruning strategy and pixel-wise loss weighting.

## 5. Rendering and Export

After training, render the training and test sets to generate aligned PNG results for evaluation and visualization.

```bash
python render.py -m <output_dir>
```

The rendering script loads the corresponding model and camera configuration and saves renders alongside ground truth images under a standard output structure.

## 6. Metric Evaluation (SSIM / PSNR / LPIPS)

A unified metric evaluation script iterates over model outputs and computes the three core metrics, supporting evaluation of multiple model directories at once.

```bash
python metrics.py --model_paths <model_dir_1> <model_dir_2> ...
```

